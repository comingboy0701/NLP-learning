{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 答疑 09.08. - assignment 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outline:\n",
    "- train a fully connected NN with tensorflow\n",
    "- add stochastic gradient descent\n",
    "- add a hidden layer using `nn.relu`\n",
    "- regularization (L2 & dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# import pickle file\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data into a shape that's more adapted to the models we are going to train:\n",
    "+ data as a flat matrix;\n",
    "+ labels as float 1-hot encodings\n",
    "\n",
    "More info on softmax regression:\n",
    "https://www.geeksforgeeks.org/softmax-regression-using-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    # One shape dimension can be -1. \n",
    "    # In this case, the value is inferred from the length of the array \n",
    "    # and remaining dimensions.\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 200000\n",
    "\n",
    "# Create graph object: instantiate\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    '''INPUT DATA'''\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    '''VARIABLES'''\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    '''TRAINING COMPUTATION'''\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized)\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    # We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = tf_train_labels))\n",
    "\n",
    "    '''OPTIMIZER'''\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    # 0.5 is the learning rate\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    '''PREDICTIONS for the training, validation, and test data.'''\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 19.6794376373291\n",
      "Training accuracy: 8.1\n",
      "Validation accuracy: 10.4\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step {}: {}'.format(step, l))\n",
    "            print('Training accuracy: {:.1f}'.format(accuracy(predictions, \n",
    "                                                              train_labels[:train_subset, :])))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            \n",
    "            # You don't have to do .eval above because we already ran the session for the\n",
    "            # train_prediction\n",
    "            print('Validation accuracy: {:.1f}'.format(accuracy(valid_prediction.eval(), \n",
    "                                                                valid_labels)))\n",
    "    print('Test accuracy: {:.1f}'.format(accuracy(test_prediction.eval(), test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to using stochastic gradient descent (SGD).\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    '''INPUT DATA'''\n",
    "    # For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    # --- **the key difference between GD and SGD**!\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    '''VARIABLES'''\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    '''TRAINING COMPUTATION'''\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized)\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "\n",
    "    '''OPTIMIZER'''\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    # 0.5 is the learning rate\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    '''PREDICTIONS for the training, validation, and test data'''\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.22445297241211\n",
      "Minibatch accuracy: 10.2\n",
      "Validation accuracy: 13.7\n",
      "Minibatch loss at step 500: 1.327433466911316\n",
      "Minibatch accuracy: 78.1\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 1000: 1.3761510848999023\n",
      "Minibatch accuracy: 71.9\n",
      "Validation accuracy: 76.6\n",
      "Minibatch loss at step 1500: 1.4343000650405884\n",
      "Minibatch accuracy: 73.4\n",
      "Validation accuracy: 77.2\n",
      "Minibatch loss at step 2000: 0.9583243131637573\n",
      "Minibatch accuracy: 82.0\n",
      "Validation accuracy: 77.0\n",
      "Minibatch loss at step 2500: 1.2906465530395508\n",
      "Minibatch accuracy: 74.2\n",
      "Validation accuracy: 77.8\n",
      "Minibatch loss at step 3000: 1.0107319355010986\n",
      "Minibatch accuracy: 76.6\n",
      "Validation accuracy: 78.2\n",
      "Test accuracy: 84.8\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem - SGD with ReLu\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [`nn.relu()`](https://www.tensorflow.org/api_docs/python/tf/nn/relu) and 1024 hidden nodes. This model should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1) # add relu layer\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels = tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for test\n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 285.65771484375\n",
      "Batch accuracy: 9.4\n",
      "Validation accuracy: 30.4\n",
      "Minibatch loss at step 500: 27.626670837402344\n",
      "Batch accuracy: 80.5\n",
      "Validation accuracy: 79.3\n",
      "Minibatch loss at step 1000: 5.7307891845703125\n",
      "Batch accuracy: 83.6\n",
      "Validation accuracy: 79.2\n",
      "Minibatch loss at step 1500: 40.735328674316406\n",
      "Batch accuracy: 71.9\n",
      "Validation accuracy: 74.9\n",
      "Minibatch loss at step 2000: 5.909951210021973\n",
      "Batch accuracy: 86.7\n",
      "Validation accuracy: 82.1\n",
      "Minibatch loss at step 2500: 11.044862747192383\n",
      "Batch accuracy: 79.7\n",
      "Validation accuracy: 81.2\n",
      "Minibatch loss at step 3000: 2.101963758468628\n",
      "Batch accuracy: 82.8\n",
      "Validation accuracy: 82.9\n",
      "Test accuracy: 88.6\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Batch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info:\n",
    "+ Comparing Stochastic Gradient Descent vs. Mini-Batch Gradient Descent:\n",
    "https://adventuresinmachinelearning.com/stochastic-gradient-descent/\n",
    "+ Implementing Shallow NN with SGD: https://leemeng.tw/using-tensorflow-to-train-a-shallow-nn-with-stochastic-gradient-descent.html\n",
    "+ Mathematical explanation for a fully-connected layer:\n",
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/fc_layer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems - Regularization\n",
    "+ 1. Adding L2 regularization\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor $t$ using [`nn.l2_loss(t)`](https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss). The right amount of regularization should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $L2 = L + \\beta\\frac{1}{2}\\Sigma|w_i|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - specify the model (the only difference is that we add a `nn.l2_loss` term to the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# take a small subset\n",
    "train_subset = 10000\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a good beta value to start with\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables    \n",
    "    # Declare the variables we want to update and optimize.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases \n",
    "    # Original loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(weights)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - run the model (similar procedures as before...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge -- try adding L2 regularization to our previous model --- SGD with ReLu hidden layer?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2. An example for extreme overfitting\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with the simple model we specified above with L2 regularization, we now run the model with a small subsample of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (500, 784) (500, 10)\n",
      "Validation set (1000, 784) (1000, 10)\n",
      "Test set (1000, 784) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_2 = train_dataset[:500, :]\n",
    "train_labels_2 = train_labels[:500]\n",
    "\n",
    "valid_dataset_2 = valid_dataset[:1000, :]\n",
    "valid_labels_2 = valid_labels[:1000, :]\n",
    "\n",
    "test_dataset_2 = test_dataset[:1000, :]\n",
    "test_labels_2 = test_labels[:1000, :]\n",
    "\n",
    "print('Training set', train_dataset_2.shape, train_labels_2.shape)\n",
    "print('Validation set', valid_dataset_2.shape, valid_labels_2.shape)\n",
    "print('Test set', test_dataset_2.shape, test_labels_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model -- change input data to the small subsample\n",
    "beta = 0.01 # define the hyper-parameter, beta, for L2 regularization\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.constant(train_dataset_2)\n",
    "    tf_train_labels = tf.constant(train_labels_2)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset_2)\n",
    "    tf_test_dataset = tf.constant(test_dataset_2)\n",
    "    \n",
    "    # Variables    \n",
    "    # Declare the variables we want to update and optimize.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases \n",
    "    # Original loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(weights)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 47.51041793823242\n",
      "Training accuracy: 10.2\n",
      "Validation accuracy: 15.2\n",
      "Loss at step 100: 10.971570014953613\n",
      "Training accuracy: 90.8\n",
      "Validation accuracy: 68.4\n",
      "Loss at step 200: 4.081431865692139\n",
      "Training accuracy: 97.2\n",
      "Validation accuracy: 71.9\n",
      "Loss at step 300: 1.6595813035964966\n",
      "Training accuracy: 99.0\n",
      "Validation accuracy: 74.5\n",
      "Loss at step 400: 0.7914624214172363\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 75.9\n",
      "Loss at step 500: 0.4788540005683899\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.8\n",
      "Loss at step 600: 0.3658328652381897\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 700: 0.32476598024368286\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.6\n",
      "Loss at step 800: 0.3097442388534546\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.8\n",
      "Loss at step 900: 0.30419057607650757\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.7\n",
      "Loss at step 1000: 0.3020966947078705\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.7\n",
      "Loss at step 1100: 0.30127716064453125\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.7\n",
      "Loss at step 1200: 0.3009336590766907\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.6\n",
      "Loss at step 1300: 0.30077269673347473\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.7\n",
      "Loss at step 1400: 0.3006853461265564\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.7\n",
      "Loss at step 1500: 0.30063027143478394\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.8\n",
      "Loss at step 1600: 0.30059128999710083\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 1700: 0.3005615472793579\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 1800: 0.30053800344467163\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 1900: 0.30051887035369873\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 77.1\n",
      "Loss at step 2000: 0.300503134727478\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 77.0\n",
      "Loss at step 2100: 0.3004900813102722\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2200: 0.30047929286956787\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2300: 0.3004702925682068\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2400: 0.30046266317367554\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2500: 0.30045634508132935\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2600: 0.30045101046562195\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2700: 0.3004465103149414\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2800: 0.30044275522232056\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 2900: 0.3004395365715027\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Loss at step 3000: 0.3004368543624878\n",
      "Training accuracy: 99.4\n",
      "Validation accuracy: 76.9\n",
      "Test accuracy: 82.0\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step {}: {}'.format(step, l))\n",
    "            print('Training accuracy: {:.1f}'.format(accuracy(predictions, \n",
    "                                                              train_labels_2)))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            \n",
    "            # You don't have to do .eval above because we already ran the session for the\n",
    "            # train_prediction\n",
    "            print('Validation accuracy: {:.1f}'.format(accuracy(valid_prediction.eval(), \n",
    "                                                                valid_labels_2)))\n",
    "    print('Test accuracy: {:.1f}'.format(accuracy(test_prediction.eval(), test_labels_2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme overfitting -- training accuracy is high while validation and test accuracy is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 3. Introducing dropout\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind \"dropout\" --- \"dropout\" means we randomly deactivate a certain number of neurons in the network during training, so it is like we are drawing a random \"sample\" from different network structures and by \"averaging\" the predictions produced by those different networks, we reduce the error related to overfitting (i.e., the model becomes more generalizable on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model -- a fully connected graph + a hiden ReLu layer (with dropout)\n",
    "num_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
    "    \n",
    "    logits_2 = tf.matmul(relu_layer_dropout, weights_2) + biases_2\n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels = tf_train_labels))\n",
    "    # Loss function with L2 Regularization with beta=0.01\n",
    "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for test\n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3674.429443359375\n",
      "Minibatch accuracy: 11.7\n",
      "Validation accuracy: 30.6\n",
      "Minibatch loss at step 500: 21.52471923828125\n",
      "Minibatch accuracy: 83.6\n",
      "Validation accuracy: 84.2\n",
      "Minibatch loss at step 1000: 1.0514339208602905\n",
      "Minibatch accuracy: 82.8\n",
      "Validation accuracy: 83.8\n",
      "Minibatch loss at step 1500: 0.9212013483047485\n",
      "Minibatch accuracy: 77.3\n",
      "Validation accuracy: 83.5\n",
      "Minibatch loss at step 2000: 0.7128937840461731\n",
      "Minibatch accuracy: 86.7\n",
      "Validation accuracy: 83.5\n",
      "Minibatch loss at step 2500: 0.8624097108840942\n",
      "Minibatch accuracy: 79.7\n",
      "Validation accuracy: 83.2\n",
      "Minibatch loss at step 3000: 0.8109461069107056\n",
      "Minibatch accuracy: 82.8\n",
      "Validation accuracy: 83.2\n",
      "Test accuracy: 89.6\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info:\n",
    "+ Illustrating the difference between L1 and L2 regularization:\n",
    "https://towardsdatascience.com/regularization-the-path-to-bias-variance-trade-off-b7a7088b4577\n",
    "\n",
    "+ Demonstration of regularizationn with TensorFlow & an example for multi-layer NN:\n",
    "https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/\n",
    "\n",
    "+ A code-efficient way to build a more complex multi-layer NN -- using [Keras](https://www.tensorflow.org/beta/guide/keras/overview)\n",
    "\n",
    "+ Hinton's original paper on the logic behind using random \"dropout\" as a technique to overcome overfitting:\n",
    "https://arxiv.org/pdf/1207.0580.pdf\n",
    "\n",
    "\n",
    "+ Comparing different learning rates with different optimizers:\n",
    "https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
